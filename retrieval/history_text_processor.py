# -*- coding: utf-8 -*-
"""
V9 (é€‚é…æ–°æ•°æ®æ ¼å¼):
- ç§»é™¤å‘é‡ç¼–ç åŠŸèƒ½ï¼ˆä¸å†ä¾èµ– TextEncoderï¼‰
- é€‚é… make_csv.py è¾“å‡ºçš„æ–°æ ¼å¼ï¼šreport å­—æ®µå·²æ˜¯æ¸…æ´—åçš„çº¯æ–‡æœ¬
- åªç”Ÿæˆå†å²æ–‡æœ¬æ–‡ä»¶ä¾›è®­ç»ƒæ—¶ä½¿ç”¨
- ä¿ç•™å†å²è¯Šæ–­å’Œå†å²æŠ¥å‘Šçš„ä¸¤éƒ¨åˆ†ç»“æ„
"""
import pandas as pd
import json
from tqdm import tqdm
import sys
import os
import logging
from typing import Set

# --- é…ç½®åŒº ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®è¦åŠ è½½çš„æœ€è¿‘å†å²æŠ¥å‘Šçš„æ•°é‡
NUM_PAST_REPORTS_TO_LOAD = 2
# è®¾ç½®æœ€å¤šä¿ç•™çš„ç›¸å…³è¯Šæ–­æ•°é‡
MAX_PAST_DIAGNOSES = 30

# V7: å¿ƒè‚ºç›¸å…³çš„å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºè¿‡æ»¤è¯Šæ–­ï¼‰
RELEVANCE_KEYWORDS: Set[str] = {
    # å¿ƒè„ (Cardiac)
    'heart', 'cardiac', 'cardio', 'myocardial', 'pericardium', 'coronary', 'atrial', 'aortic', 'mitral', 'valve',
    'cardiomegaly', 'hypertension', 'aneurysm',
    
    # è‚º/èƒ¸ (Pulmonary/Thoracic)
    'lung', 'pulmonary', 'pneumonia', 'thoracic', 'pleural', 'pneumothorax', 'bronchus', 'trachea',
    'edema', 'effusion', 'embolism', 'atelectasis', 'consolidation', 'opacity', 'hilar', 'mediastinal',
    'diaphragm', 'esophagus', 'rib', 'spine', 'clavicle', 'sternum',
    
    # å‘¼å¸ (Respiratory)
    'respiratory', 'breath', 'airway', 'dyspnea', 'apnea', 'hypoxia', 'hypoxemia',
    
    # æ…¢æ€§è‚ºéƒ¨ç–¾ç—… (Chronic Lung Disease)
    'copd', 'emphysema', 'asthma', 'bronchiectasis', 'fibrosis', 'interstitial', 
    'tuberculosis', 'tb',
    
    # è‚¿ç˜¤/å ä½ (Tumor/Mass)
    'cancer', 'carcinoma', 'tumor', 'mass', 'nodule', 'neoplasm', 'malignancy',
    'metastasis', 'metastatic', 'nodular',
    
    # è‚¾è„ (Renal - ç»å¸¸ä¸å¿ƒè¡°ç›¸å…³)
    'renal', 'kidney',
    
    # è¡€ç®¡ (Vascular)
    'vascular', 'vessel', 'aorta', 'svc', 'venous', 'artery',
    'thrombus', 'thrombosis',
    
    # å¸¸è§å½±åƒå­¦è¡¨ç° (Common Imaging Findings)
    'infiltrate', 'infiltration', 'calcification', 'calcified',
    'cyst', 'cystic', 'cavity', 'reticular',
    'congestion', 'hemorrhage', 'bleeding',
    'infarction', 'ischemia', 'ischemic',
    'inflammation', 'inflammatory', 'infection',
    'fracture', 'enlarged', 'enlargement',
    
    # æ·‹å·´ç³»ç»Ÿ (Lymphatic)
    'lymph', 'lymphadenopathy',
    
    # ç—‡çŠ¶ä¸è¯Šæ–­ (Symptoms & Diagnosis)
    'failure', 'disease', 'pain', 'shortness',
    'acute', 'chronic', 'abnormal', 'lesion'
}
# --- é…ç½®åŒºç»“æŸ ---


class HistoryRetriever:
    """
    V9: ç®€åŒ–ç‰ˆå†å²æ£€ç´¢å™¨
    åªè´Ÿè´£æå–å’Œç»„ç»‡å†å²æ–‡æœ¬ï¼Œä¸å†è¿›è¡Œå‘é‡ç¼–ç 
    """
    def __init__(self, csv_path: str):
        self.required_columns = ['study_id', 'subject_id', 'study_datetime', 'history', 'report']
        try:
            logger.info("æ­£åœ¨åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†...")
            df = pd.read_csv(csv_path)
            self._validate_dataframe(df)
            df['study_datetime'] = pd.to_datetime(df['study_datetime'], errors='coerce')
            df = df.sort_values(by=['subject_id', 'study_datetime']).reset_index(drop=True)
            self.df = df
            self.df['study_id'] = self.df['study_id'].astype('Int64')
            self.study_id_to_idx = {
                int(row['study_id']): idx 
                for idx, row in self.df.iterrows() 
                if pd.notna(row['study_id'])
            }
            logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(self.df)} æ¡è®°å½•ï¼Œæœ‰æ•ˆ study_id: {len(self.study_id_to_idx)} ä¸ª")
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            sys.exit(1)
        
        logger.info(f"HistoryRetriever (V9 æ–‡æœ¬æå–ç‰ˆ) åˆå§‹åŒ–æˆåŠŸã€‚")

    def _validate_dataframe(self, df: pd.DataFrame) -> None:
        """éªŒè¯DataFrameæ˜¯å¦åŒ…å«å¿…éœ€çš„åˆ—"""
        missing_columns = [col for col in self.required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")

    def _is_relevant(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä»»ä½•å¿ƒè‚ºç›¸å…³å…³é”®è¯"""
        if not text:
            return False
        text_low = text.lower()
        for keyword in RELEVANCE_KEYWORDS:
            if keyword in text_low:
                return True
        return False

    def _parse_history_diagnoses(self, history_json_str: str) -> str:
        """
        è§£æå¹¶è¿‡æ»¤å†å²è¯Šæ–­ä¿¡æ¯
        
        Args:
            history_json_str: historyå­—æ®µçš„JSONå­—ç¬¦ä¸²
            
        Returns:
            æ ¼å¼åŒ–çš„è¯Šæ–­æ–‡æœ¬
        """
        if pd.isna(history_json_str) or not history_json_str:
            return ""
        
        try:
            history_list = json.loads(history_json_str)
            if not isinstance(history_list, list):
                return ""
            
            # æ”¶é›†æ‰€æœ‰ç›¸å…³çš„è¯Šæ–­
            relevant_diagnoses = []
            
            for admission in history_list:
                if not isinstance(admission, dict):
                    continue
                
                diagnoses = admission.get('diagnoses', [])
                if not isinstance(diagnoses, list):
                    continue
                
                for diag in diagnoses:
                    if not isinstance(diag, dict):
                        continue
                    
                    description = diag.get('description', '')
                    if not description or description == "Unknown ICD Code":
                        continue
                    
                    # è¿‡æ»¤ç›¸å…³æ€§
                    if self._is_relevant(description):
                        icd_code = diag.get('icd_code', '')
                        relevant_diagnoses.append({
                            'code': icd_code,
                            'description': description,
                            'seq': diag.get('seq_num', 999)
                        })
            
            # å»é‡ï¼ˆåŸºäºdescriptionï¼‰
            seen_descriptions = set()
            unique_diagnoses = []
            for diag in relevant_diagnoses:
                desc = diag['description']
                if desc not in seen_descriptions:
                    seen_descriptions.add(desc)
                    unique_diagnoses.append(diag)
            
            # æŒ‰seq_numæ’åºï¼Œå–å‰MAX_PAST_DIAGNOSESä¸ª
            unique_diagnoses.sort(key=lambda x: x['seq'])
            unique_diagnoses = unique_diagnoses[:MAX_PAST_DIAGNOSES]
            
            # æ ¼å¼åŒ–è¾“å‡º
            if not unique_diagnoses:
                return ""
            
            lines = []
            for diag in unique_diagnoses:
                lines.append(f"- {diag['description']}")
            
            return "\n".join(lines)
            
        except (json.JSONDecodeError, TypeError, ValueError) as e:
            logger.warning(f"è§£æhistoryå­—æ®µæ—¶å‡ºé”™: {str(e)}")
            return ""

    def _get_past_reports_text(self, current_idx: int) -> str:
        """
        è·å–å†å²å½±åƒæŠ¥å‘Šæ–‡æœ¬ï¼ˆå·²æ¸…æ´—ï¼‰
        
        Args:
            current_idx: å½“å‰è®°å½•çš„ç´¢å¼•
            
        Returns:
            æ ¼å¼åŒ–çš„å†å²æŠ¥å‘Šæ–‡æœ¬
        """
        try:
            current_row = self.df.loc[current_idx]
            current_subject_id = current_row['subject_id']
            current_datetime = current_row['study_datetime']
            
            if pd.isna(current_datetime):
                return ""
            
            # æŸ¥æ‰¾è¯¥æ‚£è€…çš„æ‰€æœ‰å†å²æŠ¥å‘Š
            past_reports_df = self.df[
                (self.df['subject_id'] == current_subject_id) & 
                (self.df['study_datetime'] < current_datetime) &
                (pd.notna(self.df['study_datetime']))
            ]
            
            if past_reports_df.empty:
                return ""
            
            # å–æœ€è¿‘çš„Nä»½æŠ¥å‘Š
            recent_past_reports_df = past_reports_df.tail(NUM_PAST_REPORTS_TO_LOAD)
            
            # æ”¶é›†æŠ¥å‘Šæ–‡æœ¬
            report_texts = []
            for _, row in recent_past_reports_df.iterrows():
                report_text = row['report']
                
                # éªŒè¯æŠ¥å‘Šæ–‡æœ¬æ˜¯å¦æœ‰æ•ˆ
                if pd.isna(report_text) or not report_text or len(str(report_text).strip()) < 10:
                    continue
                
                # ç›´æ¥ä½¿ç”¨æ¸…æ´—åçš„æ–‡æœ¬ï¼ˆä¸å†éœ€è¦JSONè§£æï¼‰
                report_texts.append(f"Past Report: {str(report_text).strip()}")
            
            return "\n\n".join(report_texts)
            
        except Exception as e:
            logger.error(f"è·å–å†å²æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
            return ""

    def generate_history_context(self, study_id: int) -> str:
        """
        ä¸ºæŒ‡å®šçš„study_idç”Ÿæˆå®Œæ•´çš„å†å²ä¸Šä¸‹æ–‡
        
        Args:
            study_id: ç ”ç©¶ID
            
        Returns:
            æ ¼å¼åŒ–çš„å†å²ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        if study_id not in self.study_id_to_idx:
            logger.warning(f"Study ID {study_id} ä¸åœ¨æ•°æ®é›†ä¸­")
            return ""
        
        idx = self.study_id_to_idx[study_id]
        current_row = self.df.loc[idx]
        
        # 1. è·å–å†å²æŠ¥å‘Š
        past_reports_text = self._get_past_reports_text(idx)
        
        # 2. è·å–å†å²è¯Šæ–­
        history_diagnoses = self._parse_history_diagnoses(current_row['history'])
        
        # 3. ç»„åˆæ–‡æœ¬
        sections = []
        
        if past_reports_text:
            sections.append(f"[HISTORICAL IMAGING REPORTS]\n{past_reports_text}")
        
        if history_diagnoses:
            sections.append(f"[PATIENT MEDICAL HISTORY]\n{history_diagnoses}")
        
        return "\n\n".join(sections) if sections else ""

    def process_and_save_all(self, output_dir: str) -> bool:
        """
        å¤„ç†æ‰€æœ‰studyå¹¶ä¿å­˜å†å²æ–‡æœ¬
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 70)
        logger.info("å¼€å§‹å¤„ç†æ‰€æœ‰å†å²ä¸Šä¸‹æ–‡...")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"å†å²æŠ¥å‘Šæ•°é‡: {NUM_PAST_REPORTS_TO_LOAD}")
        logger.info(f"æœ€å¤§è¯Šæ–­æ•°é‡: {MAX_PAST_DIAGNOSES}")
        logger.info("=" * 70)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_txt_dir = os.path.join(output_dir, 'texts')
        os.makedirs(output_txt_dir, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total': len(self.study_id_to_idx),
            'with_reports': 0,
            'with_diagnoses': 0,
            'with_both': 0,
            'empty': 0,
            'errors': 0
        }
        
        # å¤„ç†æ‰€æœ‰study
        for study_id in tqdm(self.study_id_to_idx.keys(), desc="å¤„ç†å†å²ä¸Šä¸‹æ–‡"):
            try:
                # ç”Ÿæˆå†å²æ–‡æœ¬
                history_text = self.generate_history_context(study_id)
                
                # ç»Ÿè®¡
                if not history_text:
                    stats['empty'] += 1
                    # å³ä½¿ä¸ºç©ºä¹Ÿåˆ›å»ºæ–‡ä»¶ï¼ˆå†…å®¹ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
                    history_text = ""
                else:
                    has_reports = '[HISTORICAL IMAGING REPORTS]' in history_text
                    has_diagnoses = '[PATIENT MEDICAL HISTORY]' in history_text
                    
                    if has_reports:
                        stats['with_reports'] += 1
                    if has_diagnoses:
                        stats['with_diagnoses'] += 1
                    if has_reports and has_diagnoses:
                        stats['with_both'] += 1
                
                # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
                txt_path = os.path.join(output_txt_dir, f"{study_id}.txt")
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(history_text)
                    
            except Exception as e:
                logger.error(f"å¤„ç† study_id {study_id} æ—¶å‡ºé”™: {e}")
                stats['errors'] += 1
                # å‡ºé”™æ—¶ä¹Ÿåˆ›å»ºç©ºæ–‡ä»¶
                txt_path = os.path.join(output_txt_dir, f"{study_id}.txt")
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write("")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        logger.info("=" * 70)
        logger.info("å¤„ç†å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯ï¼š")
        logger.info(f"  æ€»è®°å½•æ•°:           {stats['total']:>6,}")
        logger.info(f"  åŒ…å«å†å²æŠ¥å‘Š:       {stats['with_reports']:>6,}  ({stats['with_reports']/stats['total']*100:>5.1f}%)")
        logger.info(f"  åŒ…å«å†å²è¯Šæ–­:       {stats['with_diagnoses']:>6,}  ({stats['with_diagnoses']/stats['total']*100:>5.1f}%)")
        logger.info(f"  ä¸¤è€…éƒ½æœ‰:           {stats['with_both']:>6,}  ({stats['with_both']/stats['total']*100:>5.1f}%)")
        logger.info(f"  æ— å†å²ä¿¡æ¯:         {stats['empty']:>6,}  ({stats['empty']/stats['total']*100:>5.1f}%)")
        logger.info(f"  å¤„ç†é”™è¯¯:           {stats['errors']:>6,}")
        logger.info("=" * 70)
        logger.info(f"âœ… æ–‡æœ¬æ–‡ä»¶å·²ä¿å­˜è‡³: {output_txt_dir}")
        logger.info("=" * 70)
        
        return stats['errors'] < stats['total']  # åªè¦ä¸æ˜¯å…¨éƒ¨å¤±è´¥å°±ç®—æˆåŠŸ


def main(csv_path: str, output_dir: str) -> bool:
    """
    ä¸»å‡½æ•°
    
    Args:
        csv_path: æ•°æ®é›†CSVè·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        # åˆå§‹åŒ–æ£€ç´¢å™¨
        retriever = HistoryRetriever(csv_path)
        
        # å¤„ç†å¹¶ä¿å­˜æ‰€æœ‰å†å²æ–‡æœ¬
        success = retriever.process_and_save_all(output_dir)
        
        if success:
            logger.info("=" * 70)
            logger.info("ğŸ‰ å†å²ä¸Šä¸‹æ–‡æå–å®Œæˆï¼")
            logger.info("=" * 70)
        else:
            logger.error("=" * 70)
            logger.error("âŒ å†å²ä¸Šä¸‹æ–‡æå–å¤±è´¥")
            logger.error("=" * 70)
        
        return success
        
    except Exception as e:
        logger.error(f"ä¸»æµç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    # é…ç½®è·¯å¾„
    CSV_FILE_PATH = "data/processed_dataset.csv"
    OUTPUT_FEATURES_DIR = "retrieval/history_context"
    
    # æ‰§è¡Œä¸»å‡½æ•°
    success = main(CSV_FILE_PATH, OUTPUT_FEATURES_DIR)
    
    sys.exit(0 if success else 1)